{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we convert the code to create a classical ML classifier on top of a pre-trained protein language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following function to load part of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_sequences(data_csv, n_samples):\n",
    "    df = pd.read_csv(data_csv)\n",
    "    small_df = df.groupby('label/fitness')['sequence'].apply(lambda s: s.sample(n_samples)).reset_index()\n",
    "\n",
    "    return small_df['sequence'].tolist(), small_df['label/fitness'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the pretrained model as usual and obtain our embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "train = '../DeepLoc_cls2_data/cls2_normal_train.csv'\n",
    "valid = '../DeepLoc_cls2_data/cls2_normal_valid.csv'\n",
    "\n",
    "train_sequences, train_labels = load_sequences(train, 100)\n",
    "val_sequences, val_labels = load_sequences(train, 20)\n",
    "\n",
    "model_name = \"facebook/esm2_t6_8M_UR50D\"\n",
    "pretrained_model = EsmModel.from_pretrained(model_name)\n",
    "tokenizer = EsmTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Tokenize\n",
    "inputs = tokenizer(sequences, return_tensors='pt')\n",
    "\n",
    "# Get embeddings\n",
    "outputs = model(**inputs)\n",
    "\n",
    "embeddings = outputs.last_hidden_state # Select the appropriate output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets actually make sure we save and load the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def save_embeddings(embeddings, labels, split, output_dir='embeddings'):\n",
    "    \"\"\"Save embeddings and labels to disk\"\"\"\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M')\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Save embeddings and labels\n",
    "    np.save(f'{output_dir}/{split}_embeddings_{timestamp}.npy', embeddings)\n",
    "    np.save(f'{output_dir}/{split}_labels_{timestamp}.npy', labels)\n",
    "    \n",
    "    print(f\"Saved embeddings and labels to {output_dir}\")\n",
    "    return f'{output_dir}/embeddings_{timestamp}.npy', f'{output_dir}/labels_{timestamp}.npy'\n",
    "\n",
    "def load_embeddings(embedding_file, label_file):\n",
    "    \"\"\"Load embeddings and labels from disk\"\"\"\n",
    "    embeddings = np.load(embedding_file)\n",
    "    labels = np.load(label_file)\n",
    "    return embeddings, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def train_random_forest(X_train, X_test, y_train, y_test):\n",
    "    # Initialize and train Random Forest\n",
    "    rf_model = RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        random_state=42,\n",
    "        n_jobs=-1  # Use all available cores\n",
    "    )\n",
    "    \n",
    "    rf_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = rf_model.predict(X_test)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"Random Forest Results:\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    return rf_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def train_svm(X_train, X_test, y_train, y_test):\n",
    "    # Initialize and train SVM\n",
    "    svm_model = SVC(\n",
    "        kernel='rbf',  # You can try 'linear', 'poly', or 'sigmoid'\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    svm_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = svm_model.predict(X_test)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"SVM Results:\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    return svm_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# For Random Forest\n",
    "rf_params = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "rf_grid = GridSearchCV(RandomForestClassifier(), rf_params, cv=5)\n",
    "rf_grid.fit(X_train, y_train)\n",
    "print(\"Best RF parameters:\", rf_grid.best_params_)\n",
    "\n",
    "# For SVM\n",
    "svm_params = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['rbf', 'linear'],\n",
    "    'gamma': ['scale', 'auto', 0.1, 1]\n",
    "}\n",
    "\n",
    "svm_grid = GridSearchCV(SVC(), svm_params, cv=5)\n",
    "svm_grid.fit(X_train, y_train)\n",
    "print(\"Best SVM parameters:\", svm_grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we add the dataloader to a SVM or RF model? For RF yes, we can add trees to the forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "### hint for RF:\n",
    "\n",
    "for batch_embeddings, batch_labels in dataloader:\n",
    "    # Convert to numpy and scale\n",
    "    batch_embeddings = scaler.transform(batch_embeddings.numpy())\n",
    "    batch_labels = batch_labels.numpy()\n",
    "    \n",
    "    # Train subset of trees\n",
    "    batch_rf = RandomForestClassifier(n_estimators=trees_per_batch,\n",
    "                                    random_state=42+current_tree,\n",
    "                                    n_jobs=-1)\n",
    "    batch_rf.fit(batch_embeddings, batch_labels)\n",
    "    \n",
    "    # Add trees to main forest\n",
    "    if current_tree == 0:\n",
    "        rf_model.estimators_ = batch_rf.estimators_\n",
    "    else:\n",
    "        rf_model.estimators_.extend(batch_rf.estimators_)\n",
    "    \n",
    "    current_tree += trees_per_batch\n",
    "    print(f\"Processed batch, total trees: {len(rf_model.estimators_)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Lets make comparisons using plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, precision_recall_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "def plot_model_comparison(models_dict, X_test, y_test, figsize=(15, 10)):\n",
    "    \"\"\"\n",
    "    Compare multiple models using ROC curves and Precision-Recall curves\n",
    "    \n",
    "    Parameters:\n",
    "    models_dict: dictionary of format {'model_name': model_object}\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)\n",
    "    \n",
    "    # ROC Curve\n",
    "    for name, model in models_dict.items():\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        ax1.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.2f})')\n",
    "    \n",
    "    ax1.plot([0, 1], [0, 1], 'k--')\n",
    "    ax1.set_xlabel('False Positive Rate')\n",
    "    ax1.set_ylabel('True Positive Rate')\n",
    "    ax1.set_title('ROC Curves')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Precision-Recall Curve\n",
    "    for name, model in models_dict.items():\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "        precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "        pr_auc = auc(recall, precision)\n",
    "        \n",
    "        ax2.plot(recall, precision, label=f'{name} (AUC = {pr_auc:.2f})')\n",
    "    \n",
    "    ax2.set_xlabel('Recall')\n",
    "    ax2.set_ylabel('Precision')\n",
    "    ax2.set_title('Precision-Recall Curves')\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrices(models_dict, X_test, y_test, figsize=(12, 5)):\n",
    "    \"\"\"\n",
    "    Plot confusion matrices for multiple models\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, len(models_dict), figsize=figsize)\n",
    "    \n",
    "    for i, (name, model) in enumerate(models_dict.items()):\n",
    "        y_pred = model.predict(X_test)\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        \n",
    "        sns.heatmap(cm, annot=True, fmt='d', ax=axes[i], cmap='Blues')\n",
    "        axes[i].set_title(f'{name} Confusion Matrix')\n",
    "        axes[i].set_xlabel('Predicted')\n",
    "        axes[i].set_ylabel('True')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_prediction_distribution(models_dict, X_test, y_test, figsize=(12, 5)):\n",
    "    \"\"\"\n",
    "    Plot distribution of prediction probabilities for each class\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, len(models_dict), figsize=figsize)\n",
    "    \n",
    "    for i, (name, model) in enumerate(models_dict.items()):\n",
    "        y_pred_proba = model.predict_proba(X_test)\n",
    "        \n",
    "        for j in range(y_pred_proba.shape[1]):\n",
    "            sns.kdeplot(data=y_pred_proba[:, j][y_test == j], \n",
    "                       ax=axes[i], \n",
    "                       label=f'Class {j}')\n",
    "        \n",
    "        axes[i].set_title(f'{name} Prediction Distribution')\n",
    "        axes[i].set_xlabel('Prediction Probability')\n",
    "        axes[i].set_ylabel('Density')\n",
    "        axes[i].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_cross_validation_comparison(models_dict, X, y, cv=5, figsize=(10, 6)):\n",
    "    \"\"\"\n",
    "    Compare cross-validation scores across models\n",
    "    \"\"\"\n",
    "    cv_scores = {}\n",
    "    for name, model in models_dict.items():\n",
    "        scores = cross_val_score(model, X, y, cv=cv)\n",
    "        cv_scores[name] = scores\n",
    "    \n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.boxplot([scores for scores in cv_scores.values()], labels=cv_scores.keys())\n",
    "    plt.title('Cross-validation Score Comparison')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# After training your models:\n",
    "models = {\n",
    "    'Random Forest': rf_model,\n",
    "    'SVM': svm_model\n",
    "}\n",
    "\n",
    "# Plot comparisons\n",
    "plot_model_comparison(models, val_embeddings, val_labels)\n",
    "plot_confusion_matrices(models, val_embeddings, val_labels)\n",
    "plot_prediction_distribution(models, val_embeddings, val_labels)\n",
    "plot_cross_validation_comparison(models, val_embeddings, val_labels)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
